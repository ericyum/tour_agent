import sqlite3
import json
import os
import gradio as gr
import re
import math
import asyncio
import io
import pandas as pd
from collections import Counter
from datetime import datetime, timedelta
# --- Environment Setup (must be first) ---
from src.infrastructure.config.settings import setup_environment, get_google_api_key, Settings
setup_environment()

# --- Matplotlib Backend Setup ---
import matplotlib
matplotlib.use('Agg')

# --- Visualization and NLP Libraries (Assumed to be installed) ---
try:
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    from matplotlib import font_manager
    from konlpy.tag import Okt
    from PIL import Image, ImageDraw, ImageFont
    import numpy as np
except ImportError:
    print("Warning: `wordcloud`, `matplotlib`, `konlpy`, `Pillow`, or `numpy` libraries are not installed. Visual analysis will not work.")
    WordCloud, plt, Okt, Image, ImageDraw, ImageFont, np = [None]*7

# --- Custom Module Imports ---
from src.infrastructure.persistence.database import get_db_connection, init_db
from src.application.supervisors.naver_review_supervisor import NaverReviewSupervisor
from src.infrastructure.external_services.naver_search.naver_review_api import get_naver_trend, search_naver_blog

# --- New LLM-related Imports (for sentiment analysis only) ---
from src.application.core.state import LLMGraphState
from src.domain.knowledge_base import knowledge_base
from src.infrastructure.llm_client import get_llm_client
from src.infrastructure.dynamic_scorer import SimpleScorer
from src.application.agents.common.content_validator import agent_content_validator
from src.application.agents.common.llm_summarizer import agent_llm_summarizer
from src.application.agents.common.rule_scorer import agent_rule_scorer_on_summary
from src.application.core.graph import app_llm_graph
from src.infrastructure.reporting.charts import create_donut_chart, create_stacked_bar_chart, create_sentence_score_bar_chart
from src.infrastructure.reporting.wordclouds import create_sentiment_wordclouds
from src.application.core.utils import get_season, save_df_to_csv, summarize_negative_feedback, create_driver, change_page, get_logger, haversine

# --- Selenium Imports (for sentiment analysis only) ---
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import traceback

# --- NLTK Setup ---
import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

# --- Constants and Setup ---
script_dir = os.path.dirname(__file__)


CUSTOM_CSS = """#gallery .thumbnail-item { max-width: 250px !important; min-width: 200px !important; flex-grow: 1 !important; }"""

from src.application.core.constants import AREA_CODE_MAP, SIGUNGU_CODE_MAP, CATEGORY_TO_ICON_MAP, NO_IMAGE_URL, PAGE_SIZE

# --- Data Loading and Mappings ---

CAT_NAME_TO_CODE = {'main': {}, 'medium': {}, 'small': {}}
TITLE_TO_CAT_NAMES = {}

COLUMN_TRANSLATIONS = {
    'addr1': 'ì£¼ì†Œ', 'addr2': 'ìƒì„¸ì£¼ì†Œ', 'tel': 'ì „í™”ë²ˆí˜¸', 'title': 'ì œëª©', 'zipcode': 'ìš°í¸ë²ˆí˜¸', 
    'telname': 'ì—°ë½ì²˜ ì´ë¦„', 'homepage': 'í™ˆí˜ì´ì§€', 'overview': 'ê°œìš”', 'sponsor1': 'ì£¼ìµœì',
    'sponsor1tel': 'ì£¼ìµœì ì—°ë½ì²˜', 'eventenddate': 'í–‰ì‚¬ ì¢…ë£Œì¼', 'playtime': 'ê³µì—° ì‹œê°„',
    'eventplace': 'í–‰ì‚¬ ì¥ì†Œ', 'eventstartdate': 'í–‰ì‚¬ ì‹œì‘ì¼', 'usetimefestival': 'ì´ìš© ìš”ê¸ˆ',
    'sponsor2': 'í›„ì›ì‚¬', 'progresstype': 'ì§„í–‰ ìƒíƒœ', 'festivaltype': 'ì¶•ì œ ìœ í˜•',
    'sponsor2tel': 'í›„ì›ì‚¬ ì—°ë½ì²˜', 'agelimit': 'ì—°ë ¹ ì œí•œ', 'spendtimefestival': 'ê´€ëŒ ì†Œìš”ì‹œê°„',
    'festivalgrade': 'ì¶•ì œ ë“±ê¸‰', 'eventhomepage': 'í–‰ì‚¬ í™ˆí˜ì´ì§€', 'subevent': 'ë¶€ëŒ€ í–‰ì‚¬',
    'program': 'í–‰ì‚¬ í”„ë¡œê·¸ë¨', 'discountinfofestival': 'í• ì¸ ì •ë³´', 'placeinfo': 'í–‰ì‚¬ì¥ ìœ„ì¹˜ ì•ˆë‚´',
    'bookingplace': 'ì˜ˆë§¤ì²˜', 'usefee': 'ì´ìš© ìš”ê¸ˆ', 'infocenterculture': 'ë¬¸ì˜ ë° ì•ˆë‚´',
    'usetimeculture': 'ì´ìš© ì‹œê°„', 'restdateculture': 'ì‰¬ëŠ” ë‚ ', 'parkingfee': 'ì£¼ì°¨ ìš”ê¸ˆ',
    'parkingculture': 'ì£¼ì°¨ ì‹œì„¤', 'chkcreditcardculture': 'ì‹ ìš©ì¹´ë“œ ê°€ëŠ¥ ì •ë³´',
    'chkbabycarriageculture': 'ìœ ëª¨ì°¨ ëŒ€ì—¬ ì •ë³´', 'spendtime': 'ê´€ëŒ ì†Œìš”ì‹œê°„',
    'accomcountculture': 'ìˆ˜ìš©ì¸ì›', 'scale': 'ê·œëª¨', 'chkpetculture': 'ë°˜ë ¤ë™ë¬¼ ë™ë°˜ ê°€ëŠ¥ ì •ë³´',
    'discountinfo': 'í• ì¸ ì •ë³´', 'distance': 'ì´ ê±°ë¦¬', 'schedule': 'ì½”ìŠ¤ ì¼ì •',
    'taketime': 'ì´ ì†Œìš”ì‹œê°„', 'theme': 'ì½”ìŠ¤ í…Œë§ˆ', 'subnum': 'ì„¸ë¶€ ì½”ìŠ¤ ë²ˆí˜¸',
    'subname': 'ì„¸ë¶€ ì½”ìŠ¤ëª…', 'subdetailoverview': 'ì„¸ë¶€ ì½”ìŠ¤ ê°œìš”', 'firstimage': 'ëŒ€í‘œ ì´ë¯¸ì§€',
    'firstimage2': 'ì¶”ê°€ ì´ë¯¸ì§€'
}

settings = Settings()
logger = get_logger(__name__)

# Initialize the database
init_db()

# Load festival categories and maps
def load_festival_categories_and_maps():
    global TITLE_TO_CAT_NAMES
    festivals_dir = os.path.join(script_dir, "festivals")
    all_categories = {}
    try:
        for filename in os.listdir(festivals_dir):
            if filename.endswith(".json"):
                with open(os.path.join(festivals_dir, filename), 'r', encoding='utf-8') as f:
                    all_categories.update(json.load(f))
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error loading festival categories: {e}")
        return {}

    for main_name, med_dict in all_categories.items():
        for med_name, small_dict in med_dict.items():
            for small_name, titles in small_dict.items():
                for title in titles:
                    TITLE_TO_CAT_NAMES[title] = (main_name, med_name, small_name)

    db_path = os.path.join(script_dir, "tour.db")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT title, cat1, cat2, cat3 FROM festivals WHERE cat1 IS NOT NULL AND cat2 IS NOT NULL AND cat3 IS NOT NULL")
    db_festivals = cursor.fetchall()
    conn.close()

    for row in db_festivals:
        title, code1, code2, code3 = row
        if title in TITLE_TO_CAT_NAMES:
            name1, name2, name3 = TITLE_TO_CAT_NAMES[title]
            CAT_NAME_TO_CODE['main'][name1] = code1
            CAT_NAME_TO_CODE['medium'][name2] = code2
            CAT_NAME_TO_CODE['small'][name3] = code3
    
    print("[app.py] Category name-to-code maps created.")
    return all_categories

ALL_FESTIVAL_CATEGORIES = load_festival_categories_and_maps()
naver_supervisor = NaverReviewSupervisor()
okt = Okt() if Okt else None

def get_korean_font():
    if plt is None: return None
    try:
        font_path = font_manager.findfont(font_manager.FontProperties(family='Malgun Gothic'))
        if os.path.exists(font_path):
            return font_path
    except:
        pass
    font_list = font_manager.findSystemFonts(fontpaths=None, fontext='ttf')
    for font in font_list:
        if 'gothic' in font.lower() or 'gulim' in font.lower() or 'apple' in font.lower():
            return font
    print("Warning: Korean font not found. Visualization text may be broken.")
    return None

KOREAN_FONT_PATH = get_korean_font()



from src.application.supervisors.db_search_supervisor import db_search_graph

def display_page(results, page):
    page = int(page)
    start_index = (page - 1) * PAGE_SIZE
    end_index = start_index + PAGE_SIZE
    page_results = results[start_index:end_index]
    gallery_output = [(item[1], item[0]) for item in page_results]
    total_pages = math.ceil(len(results) / PAGE_SIZE)
    return gallery_output, f"{page} / {total_pages}"

def display_paginated_gallery(results, page_str, direction):
    page = int(page_str.split('/')[0].strip())
    total_pages = math.ceil(len(results) / PAGE_SIZE)
    new_page = page + direction
    if 1 <= new_page <= total_pages:
        return display_page(results, new_page)
    return gr.update(), gr.update()

def search_festival_in_db(festival_name):
    if not festival_name: return None
    db_path = os.path.join(script_dir, "tour.db")
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT * FROM festivals WHERE title = ?", (festival_name,))
        festival = cursor.fetchone()
        return dict(festival) if festival else None
    finally:
        conn.close()

def display_festival_details(evt: gr.SelectData, results, page_str):
    page = int(page_str.split('/')[0].strip())
    global_index = (page - 1) * PAGE_SIZE + evt.index
    selected_title = results[global_index][0]
    details = search_festival_in_db(selected_title)

    if not details:
        return gr.update(value="ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."), None, None

    details_list = []
    exclude_cols = [
        'id', 'contentid', 'contenttypeid', 'lDongRegnCd', 'lDongSignguCd', 
        'lclsSystm1', 'lclsSystm2', 'lclsSystm3', 'mlevel', 'cpyrhtDivCd',
        'areacode', 'cat1', 'cat2', 'cat3', 'createdtime', 'mapx', 'mapy', 
        'modifiedtime', 'sigungucode'
    ]
    for key, value in details.items():
        if key in exclude_cols:
            continue
        if value is not None and str(value).strip() != '':
            display_key = COLUMN_TRANSLATIONS.get(key, key)
            details_list.append(f"**{display_key}**: {value}")
    
    details_text = "\n\n".join(details_list)

    return gr.update(value=details_text), details.get('title'), details



async def get_naver_review_info(festival_name, num_reviews):
    if not festival_name: 
        yield gr.update(value="ë¨¼ì € ì¶•ì œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", visible=True), gr.update(visible=False)
        return
    
    yield gr.update(value=f"{festival_name} í›„ê¸° ê²€ìƒ‰ ì¤‘... ({num_reviews}ê°œ)", visible=True), gr.update(visible=True, open=True)
    summary, _ = await naver_supervisor.get_review_summary_and_tips(festival_name, num_reviews=num_reviews)
    yield gr.update(value=summary, visible=True), gr.update(visible=True, open=True)




async def generate_trend_graphs(festival_name):
    if not festival_name:
        yield gr.update(visible=False), gr.update(value="ì¶•ì œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", visible=True), None, None
        return

    if plt is None:
        yield gr.update(visible=True, open=True), gr.update(value="`matplotlib` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", visible=True), None, None
        return

    yield gr.update(visible=True, open=True), gr.update(value="íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì¤‘...", visible=True), None, None

    font_properties = font_manager.FontProperties(fname=KOREAN_FONT_PATH) if KOREAN_FONT_PATH else None
    details = search_festival_in_db(festival_name)

    # --- 1. 1-Year Trend Graph ---
    today = datetime.today()
    start_date_yearly = today - timedelta(days=365)
    trend_data_yearly = get_naver_trend(festival_name, start_date_yearly, today)
    
    fig_trend_yearly, ax_yearly = plt.subplots(figsize=(10, 5))
    if trend_data_yearly: # Check if list is not empty
        df = pd.DataFrame(trend_data_yearly)
        df['period'] = pd.to_datetime(df['period'])
        ax_yearly.plot(df['period'], df['ratio'])
        ax_yearly.set_title(f"'{festival_name}' ìµœê·¼ 1ë…„ ê²€ìƒ‰ëŸ‰ íŠ¸ë Œë“œ", fontproperties=font_properties, fontsize=16)
        ax_yearly.tick_params(axis='x', rotation=30)
    else:
        ax_yearly.text(0.5, 0.5, "íŠ¸ë Œë“œ ë°ì´í„° ì—†ìŒ", ha='center', va='center', fontproperties=font_properties)
    plt.tight_layout()
    buf_trend_yearly = io.BytesIO()
    fig_trend_yearly.savefig(buf_trend_yearly, format='png')
    trend_image_yearly = Image.open(buf_trend_yearly)
    plt.close(fig_trend_yearly)

    # --- 2. Event-Period Trend Graph ---
    fig_trend_event, ax_event = plt.subplots(figsize=(10, 5))
    if details and details.get('eventstartdate'):
        try:
            date_str = str(int(details.get('eventstartdate')))
        except (ValueError, TypeError):
            date_str = str(details.get('eventstartdate'))
        
        center_date = pd.to_datetime(date_str, errors='coerce')
        
        if pd.notna(center_date):
            graph_start = center_date - timedelta(days=7)
            graph_end = center_date + timedelta(days=7)

            trend_data_event = get_naver_trend(festival_name, graph_start, graph_end)
            if trend_data_event: # Check if list is not empty
                df_event = pd.DataFrame(trend_data_event)
                df_event['period'] = pd.to_datetime(df_event['period'])
                ax_event.plot(df_event['period'], df_event['ratio'])
                ax_event.axvline(x=center_date, color='r', linestyle='--', label='Festival Start')
                ax_event.legend()
                ax_event.tick_params(axis='x', rotation=30)
            else:
                 ax_event.text(0.5, 0.5, "ê¸°ê°„ íŠ¸ë Œë“œ ë°ì´í„° ì—†ìŒ", ha='center', va='center', fontproperties=font_properties)
        else:
            ax_event.text(0.5, 0.5, "ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜", ha='center', va='center', fontproperties=font_properties)
    else:
        ax_event.text(0.5, 0.5, "ì¶•ì œ ì‹œì‘ì¼ ì •ë³´ ì—†ìŒ", ha='center', va='center', fontproperties=font_properties)
    
    ax_event.set_title(f"'{festival_name}' ì¶•ì œ ì‹œì‘ì¼ ì¤‘ì‹¬ íŠ¸ë Œë“œ", fontproperties=font_properties, fontsize=16)
    plt.tight_layout()
    buf_trend_event = io.BytesIO()
    fig_trend_event.savefig(buf_trend_event, format='png')
    trend_image_event = Image.open(buf_trend_event)
    plt.close(fig_trend_event)

    yield gr.update(visible=True, open=True), gr.update(visible=False), trend_image_yearly, trend_image_event



async def generate_word_cloud(festival_name, num_reviews):
    if not festival_name:
        yield gr.update(visible=False), gr.update(value="ì¶•ì œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", visible=True), None
        return

    if WordCloud is None or Okt is None or np is None:
        yield gr.update(visible=True, open=True), gr.update(value="`wordcloud`, `konlpy`, ë˜ëŠ” `numpy` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", visible=True), None
        return

    yield gr.update(visible=True, open=True), gr.update(value=f"ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘... ({num_reviews}ê°œ)", visible=True), None

    main_cat_tuple = TITLE_TO_CAT_NAMES.get(festival_name)
    main_cat = main_cat_tuple[0] if main_cat_tuple else None
    icon_name = None
    if main_cat:
        normalized_cat_name = main_cat.replace(" ", "")
        icon_name = CATEGORY_TO_ICON_MAP.get(normalized_cat_name)

    mask_array = None
    if icon_name:
        path = os.path.join(script_dir, "assets", f"{icon_name}.png")
        if os.path.exists(path):
            try:
                icon_image = Image.open(path).convert("L")
                mask_array = np.array(icon_image)
                mask_array = 255 - mask_array # This inverts the mask
                print(f"DEBUG: Mask array min value: {mask_array.min()}, max value: {mask_array.max()}")
                non_white_pixels = np.sum(mask_array < 255)
                total_pixels = mask_array.size
                print(f"DEBUG: Percentage of non-white pixels in mask: {non_white_pixels / total_pixels * 100:.2f}%")
            except Exception as e:
                print(f"Error loading mask image: {e}")

    stopwords = {'ì¶•ì œ', 'ì˜¤ëŠ˜', 'ì—¬ê¸°', 'ì €í¬', 'ì´ë²ˆ', 'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'í›„ê¸°', 'ìœ„í•´', 'ë•Œë¬¸', 'í•˜ë‚˜'}
    _, review_texts = await naver_supervisor.get_review_summary_and_tips(festival_name, num_reviews=num_reviews, return_full_text=True)
    
    wc_image = None
    if review_texts:
        nouns = [word for text in review_texts for word in okt.nouns(text) if len(word) > 1 and word not in stopwords]
        counts = Counter(nouns)
        if counts:
            wc = WordCloud(font_path=KOREAN_FONT_PATH, background_color="white", mask=mask_array, contour_color='steelblue', contour_width=1).generate_from_frequencies(counts)
            wc_image = wc.to_image()
    
    if wc_image is None:
        wc_image = Image.new('RGB', (800, 400), 'white')
        draw = ImageDraw.Draw(wc_image)
        try: font = ImageFont.truetype(KOREAN_FONT_PATH, 20)
        except: font = ImageFont.load_default()
        draw.text((300, 180), "ì¶”ì¶œëœ ë‹¨ì–´ ì—†ìŒ", font=font, fill="black")

    yield gr.update(visible=True, open=True), gr.update(visible=False), wc_image


async def analyze_sentiment(festival_name, num_reviews):
    outputs_to_clear = [
        gr.update(open=True),  # sentiment_accordion
        "",  # sentiment_status
        gr.update(visible=False),  # sentiment_negative_summary
        gr.update(visible=False),  # sentiment_overall_chart
        gr.update(visible=False),  # sentiment_summary
        gr.update(visible=False),  # sentiment_overall_csv
        gr.update(visible=False),  # sentiment_spring_chart
        gr.update(visible=False),  # sentiment_summer_chart
        gr.update(visible=False),  # sentiment_autumn_chart
        gr.update(visible=False),  # sentiment_winter_chart
        gr.update(visible=False),  # sentiment_spring_pos_wc
        gr.update(visible=False),  # sentiment_spring_neg_wc
        gr.update(visible=False),  # sentiment_summer_pos_wc
        gr.update(visible=False),  # sentiment_summer_neg_wc
        gr.update(visible=False),  # sentiment_autumn_pos_wc
        gr.update(visible=False),  # sentiment_autumn_neg_wc
        gr.update(visible=False),  # sentiment_winter_pos_wc
        gr.update(visible=False),  # sentiment_winter_neg_wc
        None,  # sentiment_df_output
        None,  # blog_results_df_state
        None,  # blog_judgments_state
        1,  # sentiment_blog_page_num_input
        "/ 1",  # sentiment_blog_total_pages_output
        gr.update(visible=False),  # sentiment_blog_list_csv
        gr.update(visible=False),  # sentiment_individual_summary
        gr.update(visible=False),  # sentiment_individual_donut_chart
        gr.update(visible=False),  # sentiment_individual_score_chart
        gr.update(visible=False, open=False),  # sentiment_blog_detail_accordion
    ]

    if not festival_name:
        outputs_to_clear[1] = "ì¶•ì œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
        yield tuple(outputs_to_clear)
        return

    try:
        outputs_to_clear[1] = "ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘..."
        yield tuple(outputs_to_clear)

        search_keyword = f"{festival_name} í›„ê¸°"

        blog_results_list = []
        all_negative_sentences = []
        seasonal_aspect_pairs = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
        seasonal_texts = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
        seasonal_data = {"ë´„": {"pos": 0, "neg": 0}, "ì—¬ë¦„": {"pos": 0, "neg": 0}, "ê°€ì„": {"pos": 0, "neg": 0}, "ê²¨ìš¸": {"pos": 0, "neg": 0}, "ì •ë³´ì—†ìŒ": {"pos": 0, "neg": 0}}
        total_pos, total_neg, total_strong_pos, total_strong_neg = 0, 0, 0, 0
        
        api_results = search_naver_blog(search_keyword, display=num_reviews + 10)
        if not api_results:
            outputs_to_clear[1] = f"'{search_keyword}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            yield tuple(outputs_to_clear)
            return

        candidate_blogs = []
        for item in api_results:
            if "blog.naver.com" in item["link"]:
                item['title'] = re.sub(r'<[^>]+>', '', item['title']).strip()
                if item['title'] and item["link"]:
                    candidate_blogs.append(item)
            if len(candidate_blogs) >= num_reviews:
                break
        
        if not candidate_blogs:
            outputs_to_clear[1] = f"'{search_keyword}'ì— ëŒ€í•œ ìœ íš¨í•œ ë¸”ë¡œê·¸ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            yield tuple(outputs_to_clear)
            return

        valid_blogs_data = []
        blog_judgments_list = []
        all_negative_sentences = []
        seasonal_aspect_pairs = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
        seasonal_texts = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
        seasonal_data = {"ë´„": {"pos": 0, "neg": 0}, "ì—¬ë¦„": {"pos": 0, "neg": 0}, "ê°€ì„": {"pos": 0, "neg": 0}, "ê²¨ìš¸": {"pos": 0, "neg": 0}, "ì •ë³´ì—†ìŒ": {"pos": 0, "neg": 0}}
        total_pos, total_neg, total_strong_pos, total_strong_neg = 0, 0, 0, 0

        for i, blog_data in enumerate(candidate_blogs):
            outputs_to_clear[1] = f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘... ({len(valid_blogs_data)}/{num_reviews} ì™„ë£Œ, {i+1}/{len(candidate_blogs)} í™•ì¸)"
            yield tuple(outputs_to_clear)

            try:
                content, _ = await naver_supervisor._scrape_blog_content(blog_data["link"])
                if not content or "ì˜¤ë¥˜" in content or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content:
                    continue

                max_content_length = 30000
                if len(content) > max_content_length:
                    content = content[:max_content_length] + "... (ë‚´ìš© ì¼ë¶€ ìƒëµ)"

                final_state = app_llm_graph.invoke({
                    "original_text": content, "keyword": festival_name, "title": blog_data["title"],
                    "log_details": False, "re_summarize_count": 0, "is_relevant": False
                })

                if not final_state or not final_state.get("is_relevant"):
                    continue

                judgments = final_state.get("final_judgments", [])
                if not judgments:
                    continue

                season = get_season(blog_data.get('postdate', ''))
                seasonal_texts[season].append(content)
                
                aspect_pairs = final_state.get("aspect_sentiment_pairs", [])
                if aspect_pairs:
                    seasonal_aspect_pairs[season].extend(aspect_pairs)

                blog_judgments_list.append(judgments)
                pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •")
                neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •")
                
                strong_pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •" and res["score"] >= 1.0)
                strong_neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •" and res["score"] < -1.0)

                total_pos += pos_count
                total_neg += neg_count
                total_strong_pos += strong_pos_count
                total_strong_neg += strong_neg_count
                all_negative_sentences.extend([res["sentence"] for res in judgments if res["final_verdict"] == "ë¶€ì •"])
                
                seasonal_data[season]["pos"] += pos_count
                seasonal_data[season]["neg"] += neg_count
                
                sentiment_frequency = pos_count + neg_count
                sentiment_score = ((strong_pos_count - strong_neg_count) / sentiment_frequency * 50 + 50) if sentiment_frequency > 0 else 50.0
                pos_perc = (pos_count/sentiment_frequency*100) if sentiment_frequency > 0 else 0.0
                neg_perc = (neg_count/sentiment_frequency*100) if sentiment_frequency > 0 else 0.0

                blog_results_list.append({
                    "ë¸”ë¡œê·¸ ì œëª©": blog_data["title"], "ë§í¬": blog_data["link"], "ê°ì„± ë¹ˆë„": sentiment_frequency,
                    "ê°ì„± ì ìˆ˜": f"{sentiment_score:.1f}", "ê¸ì • ë¬¸ì¥ ìˆ˜": pos_count, "ë¶€ì • ë¬¸ì¥ ìˆ˜": neg_count,
                    "ê¸ì • ë¹„ìœ¨ (%)": f"{pos_perc:.1f}", "ë¶€ì • ë¹„ìœ¨ (%)": f"{neg_perc:.1f}",
                    "ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½": "\n---\n".join([f"[{res['final_verdict']}] {res['sentence']}" for res in judgments])
                })
                valid_blogs_data.append(blog_data)
            except Exception as e:
                print(f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({festival_name}, {blog_data.get('link', 'N/A')}): {e}")
                traceback.print_exc()
                continue
        
        if not valid_blogs_data:
            outputs_to_clear[1] = f"'{festival_name}'ì— ëŒ€í•œ ìœ íš¨í•œ í›„ê¸° ë¸”ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            yield tuple(outputs_to_clear)
            return

        total_sentiment_frequency = total_pos + total_neg
        total_sentiment_score = ((total_strong_pos - total_strong_neg) / total_sentiment_frequency * 50 + 50) if total_sentiment_frequency > 0 else 50.0

        neg_summary_text = summarize_negative_feedback(all_negative_sentences)
        overall_summary_text = f"""- **ê¸ì • ë¬¸ì¥ ìˆ˜**: {total_pos}ê°œ
- **ë¶€ì • ë¬¸ì¥ ìˆ˜**: {total_neg}ê°œ
- **ê°ì„±ì–´ ë¹ˆë„ (ê¸ì •+ë¶€ì •)**: {total_sentiment_frequency}ê°œ
- **ê°ì„± ì ìˆ˜**: {total_sentiment_score:.1f}ì  (0~100ì )"""

        summary_df = pd.DataFrame([{'ì¶•ì œëª…': festival_name, 'ê°ì„± ë¹ˆë„': total_sentiment_frequency, 'ê°ì„± ì ìˆ˜': f"{total_sentiment_score:.1f}", 'ê¸ì • ë¬¸ì¥ ìˆ˜': total_pos, 'ë¶€ì • ë¬¸ì¥ ìˆ˜': total_neg}])
        summary_csv = save_df_to_csv(summary_df, "overall_summary", festival_name)
        blog_df = pd.DataFrame(blog_results_list)
        blog_list_csv = save_df_to_csv(blog_df, "blog_list", festival_name)
        
        initial_page_df, current_page, total_pages_str = change_page(blog_df, 1)

        seasonal_pos_wc_paths = {}
        seasonal_neg_wc_paths = {}
        for season, pairs in seasonal_aspect_pairs.items():
            season_en = CATEGORY_TO_ICON_MAP.get(get_season(pairs[0]['postdate'] if pairs and 'postdate' in pairs[0] else '2000-01-01'))
            if pairs and season_en:
                mask_path = os.path.abspath(os.path.join(script_dir, "assets", f"mask_{season_en}.png"))
                pos_path, neg_path = create_sentiment_wordclouds(pairs, f"{festival_name}_{season}", mask_path=mask_path)
                seasonal_pos_wc_paths[season] = pos_path
                seasonal_neg_wc_paths[season] = neg_path
            else:
                seasonal_pos_wc_paths[season] = None
                seasonal_neg_wc_paths[season] = None

        yield (
            gr.update(visible=True, open=True),
            "ë¶„ì„ ì™„ë£Œ",
            gr.update(value=neg_summary_text, visible=bool(neg_summary_text)),
            gr.update(value=create_donut_chart(total_pos, total_neg, f'{festival_name} ì „ì²´ í›„ê¸° ìš”ì•½'), visible=True),
            gr.update(value=overall_summary_text, visible=True),
            gr.update(value=summary_csv, visible=summary_csv is not None),
            gr.update(value=create_stacked_bar_chart(seasonal_data.get("ë´„", {}).get("pos", 0), seasonal_data.get("ë´„", {}).get("neg", 0), "ë´„ ì‹œì¦Œ"), visible=seasonal_data.get("ë´„", {}).get("pos", 0) > 0 or seasonal_data.get("ë´„", {}).get("neg", 0) > 0),
            gr.update(value=create_stacked_bar_chart(seasonal_data.get("ì—¬ë¦„", {}).get("pos", 0), seasonal_data.get("ì—¬ë¦„", {}).get("neg", 0), "ì—¬ë¦„ ì‹œì¦Œ"), visible=seasonal_data.get("ì—¬ë¦„", {}).get("pos", 0) > 0 or seasonal_data.get("ì—¬ë¦„", {}).get("neg", 0) > 0),
            gr.update(value=create_stacked_bar_chart(seasonal_data.get("ê°€ì„", {}).get("pos", 0), seasonal_data.get("ê°€ì„", {}).get("neg", 0), "ê°€ì„ ì‹œì¦Œ"), visible=seasonal_data.get("ê°€ì„", {}).get("pos", 0) > 0 or seasonal_data.get("ê°€ì„", {}).get("neg", 0) > 0),
            gr.update(value=create_stacked_bar_chart(seasonal_data.get("ê²¨ìš¸", {}).get("pos", 0), seasonal_data.get("ê²¨ìš¸", {}).get("neg", 0), "ê²¨ìš¸ ì‹œì¦Œ"), visible=seasonal_data.get("ê²¨ìš¸", {}).get("pos", 0) > 0 or seasonal_data.get("ê²¨ìš¸", {}).get("neg", 0) > 0),
            gr.update(value=seasonal_pos_wc_paths.get("ë´„"), visible=seasonal_pos_wc_paths.get("ë´„") is not None),
            gr.update(value=seasonal_neg_wc_paths.get("ë´„"), visible=seasonal_neg_wc_paths.get("ë´„") is not None),
            gr.update(value=seasonal_pos_wc_paths.get("ì—¬ë¦„"), visible=seasonal_pos_wc_paths.get("ì—¬ë¦„") is not None),
            gr.update(value=seasonal_neg_wc_paths.get("ì—¬ë¦„"), visible=seasonal_neg_wc_paths.get("ì—¬ë¦„") is not None),
            gr.update(value=seasonal_pos_wc_paths.get("ê°€ì„"), visible=seasonal_pos_wc_paths.get("ê°€ì„") is not None),
            gr.update(value=seasonal_neg_wc_paths.get("ê°€ì„"), visible=seasonal_neg_wc_paths.get("ê°€ì„") is not None),
            gr.update(value=seasonal_pos_wc_paths.get("ê²¨ìš¸"), visible=seasonal_pos_wc_paths.get("ê²¨ìš¸") is not None),
            gr.update(value=seasonal_neg_wc_paths.get("ê²¨ìš¸"), visible=seasonal_neg_wc_paths.get("ê²¨ìš¸") is not None),
            initial_page_df, 
            blog_df, 
            blog_judgments_list, 
            current_page, 
            total_pages_str,
            gr.update(value=blog_list_csv, visible=blog_list_csv is not None),
            gr.update(visible=False), # individual_summary
            gr.update(visible=False), # individual_donut_chart
            gr.update(visible=False), # individual_score_chart
            gr.update(visible=False, open=False) # individual_detail_accordion
        )

    except Exception as e:
        print(f"ê°ì„± ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        traceback.print_exc()
        outputs_to_clear[1] = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        yield tuple(outputs_to_clear)







# --- Core Logic Functions ---





# --- Gradio Interface ---

with gr.Blocks(css=CUSTOM_CSS) as demo:
    gr.Markdown("# ì¶•ì œ ì •ë³´ ê²€ìƒ‰ ì—ì´ì „íŠ¸")

    results_state = gr.State([])
    page_state = gr.State(1)
    selected_festival_state = gr.State()
    selected_festival_details_state = gr.State()
    recommended_facilities_state = gr.State([])
    recommended_courses_state = gr.State([])

    with gr.Group():
        with gr.Row():
            area_dropdown = gr.Dropdown(label="ì‹œ/ë„", choices=["ì „ì²´"] + sorted(list(AREA_CODE_MAP.keys())), value="ì „ì²´", interactive=True)
            sigungu_dropdown = gr.Dropdown(label="ì‹œ/êµ°/êµ¬", choices=["ì „ì²´"], value="ì „ì²´", interactive=True)
        with gr.Row():
            main_cat_dropdown = gr.Dropdown(label="ëŒ€ë¶„ë¥˜", choices=["ì „ì²´"] + sorted(list(ALL_FESTIVAL_CATEGORIES.keys())), value="ì „ì²´", interactive=True)
            medium_cat_dropdown = gr.Dropdown(label="ì¤‘ë¶„ë¥˜", choices=["ì „ì²´"], value="ì „ì²´", interactive=True)
            small_cat_dropdown = gr.Dropdown(label="ì†Œë¶„ë¥˜", choices=["ì „ì²´"], value="ì „ì²´", interactive=True)
    
    search_btn = gr.Button("ê²€ìƒ‰", variant="primary")

    with gr.Column(visible=False) as results_area:
        festival_gallery = gr.Gallery(label="ì¶•ì œ ëª©ë¡", show_label=False, elem_id="gallery", columns=4, height="auto", object_fit="contain")
        with gr.Row(variant="panel"):
            prev_button = gr.Button("â—€ ì´ì „")
            page_display = gr.Textbox(value="1 / 1", label="í˜ì´ì§€", interactive=False, container=False, scale=1)
            next_button = gr.Button("ë‹¤ìŒ â–¶")

    with gr.Accordion("ì¶•ì œ ìƒì„¸ ì •ë³´", open=False) as details_accordion:
        festival_details_output = gr.Markdown()

    with gr.Accordion("ì¢Œí‘œ ê¸°ë°˜ ì¶”ì²œ", open=False, visible=False) as recommend_accordion:
        with gr.Row():
            recommend_radius_slider = gr.Slider(minimum=100, maximum=20000, value=5000, step=100, label="ë°˜ê²½ (ë¯¸í„°)", interactive=True)
            recommend_btn = gr.Button("ì¶”ì²œ ë°›ê¸°", variant="primary")
        
        with gr.Row(visible=False) as ranking_controls: # Hide until there are results
            ranking_reviews_slider = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="ìˆœìœ„ìš© ë¦¬ë·° ìˆ˜", interactive=True)
            ranking_top_n_slider = gr.Slider(minimum=1, maximum=5, value=3, step=1, label="í‘œì‹œí•  ìˆœìœ„ ìˆ˜", interactive=True)
            rank_facilities_btn = gr.Button("ê´€ê´‘ ì‹œì„¤ ìˆœìœ„ ë§¤ê¸°ê¸°")
            rank_courses_btn = gr.Button("ê´€ê´‘ ì½”ìŠ¤ ìˆœìœ„ ë§¤ê¸°ê¸°")

        recommend_status = gr.Textbox(label="ìƒíƒœ", interactive=False, visible=False)
        gr.Markdown("### ì¶”ì²œ ê´€ê´‘ ì‹œì„¤")
        recommend_facilities_gallery = gr.Gallery(label="ì¶”ì²œ ê´€ê´‘ ì‹œì„¤", show_label=False, elem_id="recommend_facilities_gallery", columns=4, height="auto", object_fit="contain")
        facility_ranking_report = gr.Markdown(visible=False)
        gr.Markdown("### ì¶”ì²œ ê´€ê´‘ ì½”ìŠ¤")
        recommend_courses_gallery = gr.Gallery(label="ì¶”ì²œ ê´€ê´‘ ì½”ìŠ¤", show_label=False, elem_id="recommend_courses_gallery", columns=4, height="auto", object_fit="contain")
        course_ranking_report = gr.Markdown(visible=False)
        with gr.Accordion("ì¶”ì²œ ì¥ì†Œ ìƒì„¸ ì •ë³´", open=False, visible=False) as recommend_details_accordion:
            recommend_details_output = gr.Markdown()

    with gr.Accordion("Naver í›„ê¸° ìš”ì•½ ë° ê¿€íŒ", open=False, visible=False) as naver_review_accordion:
        with gr.Row():
            num_reviews_naver_summary = gr.Slider(minimum=1, maximum=20, value=5, step=1, label="ë¶„ì„í•  í›„ê¸° ìˆ˜ (ë„¤ì´ë²„ ìš”ì•½)", interactive=True)
            naver_search_btn = gr.Button("ë„¤ì´ë²„ í›„ê¸° ìš”ì•½ ê²€ìƒ‰", variant="primary")
        naver_review_output = gr.Markdown()
    
    with gr.Accordion("ê²€ìƒ‰ëŸ‰ íŠ¸ë Œë“œ ê·¸ë˜í”„", open=False, visible=False) as trend_accordion:
        trend_graph_btn = gr.Button("íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„±", variant="primary")
        trend_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
        with gr.Row():
            trend_plot_yearly = gr.Image(label="ìµœê·¼ 1ë…„ ê²€ìƒ‰ëŸ‰ íŠ¸ë Œë“œ")
            trend_plot_event = gr.Image(label="ì¶•ì œ ê¸°ê°„ ì¤‘ì‹¬ íŠ¸ë Œë“œ")

    with gr.Accordion("ì›Œë“œ í´ë¼ìš°ë“œ", open=False, visible=False) as wordcloud_accordion:
        with gr.Row():
            num_reviews_wordcloud = gr.Slider(minimum=1, maximum=100, value=20, step=1, label="ë¶„ì„í•  í›„ê¸° ìˆ˜ (ì›Œë“œí´ë¼ìš°ë“œ)", interactive=True)
            word_cloud_btn = gr.Button("ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±", variant="primary")
        wordcloud_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
        wordcloud_plot = gr.Image(label="ì¶•ì œì˜ ì£¼ìš” í•µì‹¬ ìš”ì†Œë“¤")

    # --- Gradio Interface ---

    # State variables for pagination and individual blog details
    blog_results_df_state = gr.State()
    blog_judgments_state = gr.State()
    individual_blog_page_num_state = gr.State(1)

    with gr.Accordion("ê°ì„± ë¶„ì„", open=False, visible=False) as sentiment_accordion:
        with gr.Row():
            num_reviews_slider = gr.Slider(minimum=1, maximum=100, value=10, step=1, label="ë¶„ì„í•  í›„ê¸° ìˆ˜", interactive=True)
            run_sentiment_btn = gr.Button("ê°ì„± ë¶„ì„ ì‹¤í–‰", variant="primary")
        
        sentiment_status = gr.Textbox(label="ë¶„ì„ ìƒíƒœ", interactive=False)
        
        with gr.Accordion("ì¢…í•© ë¶„ì„ ê²°ê³¼", open=True):
            sentiment_summary = gr.Markdown(label="ì¢…í•© ë¶„ì„ ìƒì„¸", visible=False)
            sentiment_overall_csv = gr.File(label="ì¢…í•© ë¶„ì„ (CSV) ë‹¤ìš´ë¡œë“œ", visible=False)
            sentiment_overall_chart = gr.Plot(label="ì „ì²´ í›„ê¸° ìš”ì•½", visible=False)
            sentiment_negative_summary = gr.Markdown(label="ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½", visible=False)
            
            with gr.Accordion("ê³„ì ˆë³„ ìƒì„¸ ë¶„ì„", open=False):
                with gr.Row():
                    sentiment_spring_chart = gr.Plot(label="ë´„ ì‹œì¦Œ", visible=False, scale=1)
                    sentiment_spring_pos_wc = gr.Image(label="ë´„ ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                    sentiment_spring_neg_wc = gr.Image(label="ë´„ ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                with gr.Row():
                    sentiment_summer_chart = gr.Plot(label="ì—¬ë¦„ ì‹œì¦Œ", visible=False, scale=1)
                    sentiment_summer_pos_wc = gr.Image(label="ì—¬ë¦„ ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                    sentiment_summer_neg_wc = gr.Image(label="ì—¬ë¦„ ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                with gr.Row():
                    sentiment_autumn_chart = gr.Plot(label="ê°€ì„ ì‹œì¦Œ", visible=False, scale=1)
                    sentiment_autumn_pos_wc = gr.Image(label="ê°€ì„ ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                    sentiment_autumn_neg_wc = gr.Image(label="ê°€ì„ ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                with gr.Row():
                    sentiment_winter_chart = gr.Plot(label="ê²¨ìš¸ ì‹œì¦Œ", visible=False, scale=1)
                    sentiment_winter_pos_wc = gr.Image(label="ê²¨ìš¸ ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)
                    sentiment_winter_neg_wc = gr.Image(label="ê²¨ìš¸ ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ", visible=False, scale=1)

        gr.Markdown("### ê°œë³„ ë¸”ë¡œê·¸ ë¶„ì„ ê²°ê³¼")
        sentiment_df_output = gr.DataFrame(
            headers=["ë¸”ë¡œê·¸ ì œëª©", "ë§í¬", "ê°ì„± ë¹ˆë„", "ê°ì„± ì ìˆ˜", "ê¸ì • ë¬¸ì¥ ìˆ˜", "ë¶€ì • ë¬¸ì¥ ìˆ˜", "ê¸ì • ë¹„ìœ¨ (%)", "ë¶€ì • ë¹„ìœ¨ (%)", "ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½"],
            datatype=["str", "str", "number", "str", "number", "number", "str", "str", "str"],
            label="ê°œë³„ ë¸”ë¡œê·¸ ë¶„ì„ ê²°ê³¼", wrap=True, interactive=True
        )
        with gr.Row():
            sentiment_blog_page_num_input = gr.Number(value=1, label="í˜ì´ì§€ ë²ˆí˜¸", interactive=True, scale=1)
            sentiment_blog_total_pages_output = gr.Textbox(value="/ 1", label="ì „ì²´ í˜ì´ì§€", interactive=False, container=False, scale=1)
            sentiment_blog_list_csv = gr.File(label="ì „ì²´ ë¸”ë¡œê·¸ ëª©ë¡(CSV) ë‹¤ìš´ë¡œë“œ", visible=False, scale=2)

        with gr.Accordion("ê°œë³„ ë¸”ë¡œê·¸ ìƒì„¸ ë¶„ì„ (í‘œì—ì„œ í–‰ ì„ íƒ)", open=False, visible=False) as sentiment_blog_detail_accordion:
            sentiment_individual_summary = gr.Textbox(label="ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½", visible=False, interactive=False, lines=10)
            with gr.Row():
                sentiment_individual_donut_chart = gr.Plot(label="ê°œë³„ ë¸”ë¡œê·¸ ê¸/ë¶€ì • ë¹„ìœ¨", visible=False)
                sentiment_individual_score_chart = gr.Plot(label="ë¬¸ì¥ë³„ ê°ì„± ì ìˆ˜", visible=False)

    # --- Event Handlers ---

    def get_trend_score(keyword):
        if not keyword:
            return 0

        today = datetime.today()
        start_date = today - timedelta(days=90) # Last 90 days
        trend_data = get_naver_trend(keyword, start_date, today)
        
        if not trend_data:
            return 0
            
        df = pd.DataFrame(trend_data)
        if 'ratio' in df.columns and not df['ratio'].empty:
            # Return the average ratio as the score
            return df['ratio'].mean()
        
        return 0

    async def get_sentiment_score(keyword, num_reviews):
        if not keyword:
            return 50.0, [] # Return a neutral score and empty list

        search_keyword = f"{keyword} í›„ê¸°"
        
        api_results = search_naver_blog(search_keyword, display=num_reviews + 10)
        if not api_results:
            return 50.0, []

        candidate_blogs = []
        for item in api_results:
            if "blog.naver.com" in item["link"]:
                item['title'] = re.sub(r'<[^>]+>', '', item['title']).strip()
                if item['title'] and item["link"]:
                    candidate_blogs.append(item)
            if len(candidate_blogs) >= num_reviews:
                break
        
        if not candidate_blogs:
            return 50.0, []

        total_strong_pos = 0
        total_strong_neg = 0
        total_sentiment_frequency = 0
        all_positive_judgments = []

        for blog_data in candidate_blogs:
            try:
                content, _ = await naver_supervisor._scrape_blog_content(blog_data["link"])
                if not content or "ì˜¤ë¥˜" in content or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content:
                    continue

                max_content_length = 30000
                if len(content) > max_content_length:
                    content = content[:max_content_length]

                final_state = app_llm_graph.invoke({
                    "original_text": content, "keyword": keyword, "title": blog_data["title"],
                    "log_details": False, "re_summarize_count": 0, "is_relevant": False
                })

                if not final_state or not final_state.get("is_relevant"):
                    continue

                judgments = final_state.get("final_judgments", [])
                if not judgments:
                    continue

                all_positive_judgments.extend([j for j in judgments if j["final_verdict"] == "ê¸ì •"])

                pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •")
                neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •")
                strong_pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •" and res["score"] >= 1.0)
                strong_neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •" and res["score"] < -1.0)

                total_strong_pos += strong_pos_count
                total_strong_neg += strong_neg_count
                total_sentiment_frequency += (pos_count + neg_count)

            except Exception as e:
                print(f"Error getting sentiment for '{keyword}': {e}")
                continue
        
        if total_sentiment_frequency == 0:
            return 50.0, []
            
        sentiment_score = ((total_strong_pos - total_strong_neg) / total_sentiment_frequency * 50 + 50)
        return sentiment_score, all_positive_judgments

    async def summarize_trend_reasons(keyword):
        if not keyword:
            return "í‚¤ì›Œë“œê°€ ì—†ì–´ íŠ¸ë Œë“œ ë¶„ì„ ë¶ˆê°€"

        today = datetime.today()
        start_date = today - timedelta(days=90)
        trend_data = get_naver_trend(keyword, start_date, today)

        if not trend_data:
            return "íŠ¸ë Œë“œ ë°ì´í„° ì—†ìŒ"

        df = pd.DataFrame(trend_data)
        data_str = df.to_string()

        llm = get_llm_client(temperature=0.2)
        prompt = f"""
        ë‹¤ìŒì€ '{keyword}'ì— ëŒ€í•œ ìµœê·¼ 90ì¼ê°„ì˜ ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ íŠ¸ë Œë“œ ë°ì´í„°ì…ë‹ˆë‹¤.
        ë°ì´í„°(ë‚ ì§œë³„ ê´€ì‹¬ë„ ë¹„ìœ¨)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ê²€ìƒ‰ëŸ‰ íŠ¸ë Œë“œì˜ íŠ¹ì§•ì„ 1~2ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ì˜ˆ: 'ìµœê·¼ í•œ ë‹¬ê°„ ê´€ì‹¬ë„ê°€ ê¾¸ì¤€íˆ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.' ë˜ëŠ” 'íŠ¹ì • ë‚ ì§œì— ê²€ìƒ‰ëŸ‰ì´ ê¸‰ì¦í•˜ëŠ” íŒ¨í„´ì„ ë³´ì…ë‹ˆë‹¤.'

        ë°ì´í„°:
        {data_str}
        """
        try:
            response = await llm.ainvoke(prompt)
            return response.content.strip()
        except Exception as e:
            print(f"Error summarizing trend: {e}")
            return "íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"

    async def summarize_sentiment_reasons(positive_judgments, keyword):
        if not positive_judgments:
            return "ê¸ì • ë¦¬ë·°ê°€ ì—†ì–´ ë¶„ì„ ë¶ˆê°€"

        sentences = [j['sentence'] for j in positive_judgments]
        sentences_str = "\n- ".join(sentences[:20])

        llm = get_llm_client(temperature=0.2)
        prompt = f"""
        ë‹¤ìŒì€ '{keyword}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ë¦¬ë·°ì—ì„œ ì¶”ì¶œëœ ê¸ì •ì ì¸ ë¬¸ì¥ë“¤ì…ë‹ˆë‹¤.
        ì´ ë¬¸ì¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìë“¤ì´ ì£¼ë¡œ ì–´ë–¤ ì ì„ ì¹­ì°¬í•˜ëŠ”ì§€ í•µì‹¬ì ì¸ ì´ìœ  1~2ê°€ì§€ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ì˜ˆ: 'ê¹¨ë—í•œ ì‹œì„¤ê³¼ ë‹¤ì–‘í•œ ë¨¹ê±°ë¦¬ì— ëŒ€í•œ ì¹­ì°¬ì´ ë§ìŠµë‹ˆë‹¤.' ë˜ëŠ” 'ì•„ì´ë“¤ì´ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì²´í—˜ í”„ë¡œê·¸ë¨ì´ ì¢‹ì€ í‰ê°€ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.'

        ê¸ì • ë¬¸ì¥ ëª©ë¡:
        - {sentences_str}
        """
        try:
            response = await llm.ainvoke(prompt)
            return response.content.strip()
        except Exception as e:
            print(f"Error summarizing sentiment: {e}")
            return "ê°ì„± ë¶„ì„ ì´ìœ  ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"


    async def rank_facilities(facilities_list, num_reviews, top_n, progress=gr.Progress()):
        if not facilities_list:
            return [], "ì‹œì„¤ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", gr.update(value=[]), "", gr.update(visible=False), gr.update(visible=False)

        async def process_facility(facility):
            title = facility.get('title', '')
            trend_score = get_trend_score(title)
            sentiment_score, positive_judgments = await get_sentiment_score(title, num_reviews)
            
            trend_reason, sentiment_reason = await asyncio.gather(
                summarize_trend_reasons(title),
                summarize_sentiment_reasons(positive_judgments, title)
            )
            
            facility['trend_score'] = round(trend_score, 2)
            facility['sentiment_score'] = round(sentiment_score, 2)
            facility['ranking_score'] = round((trend_score * 0.5) + (sentiment_score * 0.5), 2)
            facility['trend_reason'] = trend_reason
            facility['sentiment_reason'] = sentiment_reason
            return facility

        tasks = [process_facility(f) for f in facilities_list]
        ranked_facilities = []
        for task in progress.tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="ì‹œì„¤ ì ìˆ˜ ê³„ì‚° ì¤‘"):
            result_facility = await task
            ranked_facilities.append(result_facility)

        ranked_facilities.sort(key=lambda x: x.get('ranking_score', 0), reverse=True)
        gallery_output = [(item.get('firstimage', NO_IMAGE_URL) or NO_IMAGE_URL, f"ì ìˆ˜: {item.get('ranking_score')} - {item['title']}") for item in ranked_facilities]
        report_update = await generate_full_report(ranked_facilities, top_n)
        
        return ranked_facilities, "ìˆœìœ„ ê³„ì‚° ì™„ë£Œ!", gallery_output, report_update

    async def rank_courses(courses_list, num_reviews, top_n, progress=gr.Progress()):
        if not courses_list:
            return [], "ì½”ìŠ¤ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", gr.update(value=[]), "", gr.update(visible=False), gr.update(visible=False)

        async def process_course(course):
            course_title = course.get('title', '')
            sub_points = course.get('sub_points', [])
            if not sub_points:
                course['ranking_score'] = 0
                return course

            sub_point_trend_scores = []
            sub_point_sentiment_scores = []
            all_positive_judgments = []

            for sub_point in sub_points:
                sub_title = sub_point.get('subname', '')
                if not sub_title:
                    continue
                
                trend_score = get_trend_score(sub_title)
                sentiment_score, positive_judgments = await get_sentiment_score(sub_title, num_reviews)
                sub_point_trend_scores.append(trend_score)
                sub_point_sentiment_scores.append(sentiment_score)
                all_positive_judgments.extend(positive_judgments)
            
            if not sub_point_trend_scores:
                course['ranking_score'] = 0
                course['trend_score'] = 0
                course['sentiment_score'] = 0
                course['trend_reason'] = "ì„¸ë¶€ ì½”ìŠ¤ ì •ë³´ ë¶€ì¡±"
                course['sentiment_reason'] = "ì„¸ë¶€ ì½”ìŠ¤ ì •ë³´ ë¶€ì¡±"
            else:
                avg_trend_score = sum(sub_point_trend_scores) / len(sub_point_trend_scores)
                avg_sentiment_score = sum(sub_point_sentiment_scores) / len(sub_point_sentiment_scores)
                course['trend_score'] = round(avg_trend_score, 2)
                course['sentiment_score'] = round(avg_sentiment_score, 2)
                course['ranking_score'] = round((avg_trend_score * 0.5) + (avg_sentiment_score * 0.5), 2)
                
                trend_reason, sentiment_reason = await asyncio.gather(
                    summarize_trend_reasons(course_title),
                    summarize_sentiment_reasons(all_positive_judgments, course_title)
                )
                course['trend_reason'] = trend_reason
                course['sentiment_reason'] = sentiment_reason
            
            return course

        tasks = [process_course(c) for c in courses_list]
        ranked_courses = []
        for task in progress.tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="ì½”ìŠ¤ ì ìˆ˜ ê³„ì‚° ì¤‘"):
            result_course = await task
            ranked_courses.append(result_course)

        ranked_courses.sort(key=lambda x: x.get('ranking_score', 0), reverse=True)
        gallery_output = [(item.get('firstimage', NO_IMAGE_URL) or NO_IMAGE_URL, f"ì ìˆ˜: {item.get('ranking_score')} - {item['title']}") for item in ranked_courses]
        report_md, report_visible, header_visible = generate_full_report(ranked_courses, top_n)

        return ranked_courses, "ìˆœìœ„ ê³„ì‚° ì™„ë£Œ!", gallery_output, report_md, report_visible, header_visible

    async def generate_full_report(ranked_list, top_n):
        top_n = int(top_n)
        if not ranked_list or not any(item.get('ranking_score', 0) > 0 for item in ranked_list[:top_n]):
            return gr.update(value="ìŠ¤ì½”ì–´ë§ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.", visible=True)

        # Generate the new comparative summary
        comparative_summary = await generate_comparative_summary(ranked_list[:top_n])

        report_parts = [f"## ğŸ† ìµœì¢… ìˆœìœ„ ë¶„ì„\n{comparative_summary}", "---"]
        top_items = ranked_list[:top_n]
        medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"]

        for i, item in enumerate(top_items):
            if i >= len(medals):
                rank_indicator = f"{i+1}ìœ„"
            else:
                rank_indicator = medals[i]

            title = item.get('title', 'N/A')
            total_score = item.get('ranking_score', 'N/A')
            trend_score = item.get('trend_score', 'N/A')
            sentiment_score = item.get('sentiment_score', 'N/A')
            image_url = item.get('firstimage', NO_IMAGE_URL) or NO_IMAGE_URL
            trend_reason = item.get('trend_reason', 'ë¶„ì„ ì •ë³´ ì—†ìŒ')
            sentiment_reason = item.get('sentiment_reason', 'ë¶„ì„ ì •ë³´ ì—†ìŒ')

            report_parts.append(f"### {rank_indicator} {i+1}ìœ„: {title} (ì¢…í•© ì ìˆ˜: {total_score})")
            report_parts.append(f"![{title}]({image_url})\n")
            report_parts.append(f"- **íŠ¸ë Œë“œ ì ìˆ˜**: {trend_score}")
            report_parts.append(f"- **ê°ì„± ì ìˆ˜**: {sentiment_score}")
            report_parts.append(f"**ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„**: {trend_reason}")
            report_parts.append(f"**â¤ï¸ ê°ì„± ë¶„ì„**: {sentiment_reason}")
            report_parts.append("---")

        report_md = "\n\n".join(report_parts)
        return gr.update(value=report_md, visible=True)

    async def generate_comparative_summary(ranked_list):
        llm = get_llm_client(temperature=0.3)
        
        # Format the data for the prompt
        data_for_prompt = []
        for item in ranked_list:
            data_for_prompt.append({
                "title": item.get('title'),
                "ranking_score": item.get('ranking_score'),
                "trend_score": item.get('trend_score'),
                "sentiment_score": item.get('sentiment_score'),
                "trend_reason": item.get('trend_reason'),
                "sentiment_reason": item.get('sentiment_reason')
            })
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì—¬í–‰ ì¶”ì²œ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì— íŠ¸ë Œë“œ ì ìˆ˜ì™€ ê°ì„± ì ìˆ˜ë¥¼ ì¢…í•©í•˜ì—¬ ìˆœìœ„ë¥¼ ë§¤ê¸´ ê´€ê´‘ì§€ ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤. 
        ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 1ìœ„ê°€ ì™œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆëŠ”ì§€ ë‹¤ë¥¸ ìˆœìœ„ì™€ ë¹„êµí•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ë‹¨ìˆœíˆ ì ìˆ˜ê°€ ë†’ë‹¤ëŠ” ì‚¬ì‹¤ë§Œ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê° ì ìˆ˜ì˜ ì˜ë¯¸(íŠ¸ë Œë“œ=í™”ì œì„±, ê°ì„±=ì‹¤ì œ ë§Œì¡±ë„)ë¥¼ í•´ì„í•˜ê³ , ë‹¤ë¥¸ ì¥ì†Œì™€ ë¹„êµí•˜ì—¬ ì„¤ë“ë ¥ ìˆëŠ” ì´ìœ ë¥¼ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

        [ë°ì´í„°]
        {json.dumps(data_for_prompt, ensure_ascii=False, indent=2)}

        [ìš”ì•½ ì˜ˆì‹œ]
        "ìµœì¢… ë¶„ì„ ê²°ê³¼, Aê°€ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤. ë¹„ë¡ Bê°€ ì‹¤ì œ ë°©ë¬¸ê°ì˜ ë§Œì¡±ë„(ê°ì„± ì ìˆ˜)ëŠ” ë” ë†’ì•˜ì§€ë§Œ, AëŠ” ì••ë„ì ì¸ í™”ì œì„±(íŠ¸ë Œë“œ ì ìˆ˜)ê³¼ ì¤€ìˆ˜í•œ ë§Œì¡±ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ê· í˜• ì¡íŒ ì¶”ì²œ ì¥ì†Œë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë°˜ë©´ CëŠ” ë†’ì€ í™”ì œì„±ì—ë„ ë¶ˆêµ¬í•˜ê³  ê¸ì •ì ì¸ í”¼ë“œë°±ì´ ë¶€ì¡±í•˜ì—¬ ìˆœìœ„ê°€ ë°€ë ¸ìŠµë‹ˆë‹¤."
        """
        try:
            response = await llm.ainvoke(prompt)
            return response.content.strip()
        except Exception as e:
            print(f"Error generating comparative summary: {e}")
            return "ìµœì¢… ë¶„ì„ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def update_sigungu(area):
        choices = ["ì „ì²´"] + sorted(list(SIGUNGU_CODE_MAP.get(area, {}).keys())) if area != "ì „ì²´" else ["ì „ì²´"]
        return gr.update(choices=choices, value="ì „ì²´")

    def update_medium_cat(main_cat):
        choices = ["ì „ì²´"] + sorted(list(ALL_FESTIVAL_CATEGORIES.get(main_cat, {}).keys())) if main_cat != "ì „ì²´" else ["ì „ì²´"]
        return gr.update(choices=choices, value="ì „ì²´")

    def update_small_cat(main_cat, medium_cat):
        choices = ["ì „ì²´"] + sorted(list(ALL_FESTIVAL_CATEGORIES.get(main_cat, {}).get(medium_cat, {}).keys())) if main_cat != "ì „ì²´" and medium_cat != "ì „ì²´" else ["ì „ì²´"]
        return gr.update(choices=choices, value="ì „ì²´")

    area_dropdown.change(fn=update_sigungu, inputs=area_dropdown, outputs=sigungu_dropdown)
    main_cat_dropdown.change(fn=update_medium_cat, inputs=main_cat_dropdown, outputs=medium_cat_dropdown)
    medium_cat_dropdown.change(fn=update_small_cat, inputs=[main_cat_dropdown, medium_cat_dropdown], outputs=small_cat_dropdown)

    def run_search_and_display(area, sigungu, main_cat, medium_cat, small_cat):
        initial_state = {
            "search_type": "festival_search",
            "area": area, "sigungu": sigungu, "main_cat": main_cat,
            "medium_cat": medium_cat, "small_cat": small_cat
        }
        final_state = db_search_graph.invoke(initial_state)
        results = final_state.get("results", [])
        total_pages = math.ceil(len(results) / PAGE_SIZE)
        
        gallery, page_str_updated = display_page(results, 1)
        return results, gallery, f"1 / {total_pages}", gr.update(visible=len(results) > 0)

    def run_nearby_search(festival_details, radius_meters):
        if not festival_details or not festival_details.get('mapx') or not festival_details.get('mapy'):
            return [], [], gr.update(value="ì¶•ì œ ì¢Œí‘œ ì •ë³´ê°€ ì—†ì–´ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", visible=True), [], [], gr.update(visible=False)

        initial_state = {
            "search_type": "nearby_search",
            "latitude": festival_details.get('mapy'),
            "longitude": festival_details.get('mapx'),
            "radius": radius_meters
        }
        final_state = db_search_graph.invoke(initial_state)
        
        facilities_recs = final_state.get("recommended_facilities", [])
        courses_recs = final_state.get("recommended_courses", [])

        if not facilities_recs and not courses_recs:
            return [], [], gr.update(value=f"{radius_meters}m ë‚´ì— ì¶”ì²œí•  ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.", visible=True), [], [], gr.update(visible=False)

        facility_gallery_output = [(item.get('firstimage', NO_IMAGE_URL) or NO_IMAGE_URL, item['title']) for item in facilities_recs]
        course_gallery_output = [(item.get('firstimage', NO_IMAGE_URL) or NO_IMAGE_URL, item['title']) for item in courses_recs]

        return facilities_recs, courses_recs, facility_gallery_output, course_gallery_output, gr.update(visible=False), gr.update(visible=True)

    search_btn.click(
        fn=run_search_and_display,
        inputs=[area_dropdown, sigungu_dropdown, main_cat_dropdown, medium_cat_dropdown, small_cat_dropdown],
        outputs=[results_state, festival_gallery, page_display, results_area]
    )

    prev_button.click(fn=lambda r, p: display_paginated_gallery(r, p, -1), inputs=[results_state, page_display], outputs=[festival_gallery, page_display])
    next_button.click(fn=lambda r, p: display_paginated_gallery(r, p, 1), inputs=[results_state, page_display], outputs=[festival_gallery, page_display])

    festival_gallery.select(
        fn=display_festival_details,
        inputs=[results_state, page_display],
        outputs=[
            festival_details_output, selected_festival_state, selected_festival_details_state
        ]
    ).then(
        fn=lambda: (
            gr.update(open=True), # details_accordion
            gr.update(visible=True), # recommend_accordion
            gr.update(visible=True), # naver_review_accordion
            gr.update(visible=True), # trend_accordion
            gr.update(visible=True), # wordcloud_accordion
            gr.update(visible=True)  # sentiment_accordion
        ),
        outputs=[
            details_accordion,
            recommend_accordion,
            naver_review_accordion,
            trend_accordion,
            wordcloud_accordion,
            sentiment_accordion
        ]
    )
    

    naver_search_btn.click(
        fn=get_naver_review_info,
        inputs=[selected_festival_state, num_reviews_naver_summary],
        outputs=[naver_review_output, naver_review_accordion]
    )

    trend_graph_btn.click(
        fn=generate_trend_graphs,
        inputs=[selected_festival_state],
        outputs=[trend_accordion, trend_status, trend_plot_yearly, trend_plot_event]
    )

    word_cloud_btn.click(
        fn=generate_word_cloud,
        inputs=[selected_festival_state, num_reviews_wordcloud],
        outputs=[wordcloud_accordion, wordcloud_status, wordcloud_plot]
    )

    run_sentiment_btn.click(
        fn=analyze_sentiment,
        inputs=[selected_festival_state, num_reviews_slider],
        outputs=[
            sentiment_accordion,
            sentiment_status,
            sentiment_negative_summary,
            sentiment_overall_chart,
            sentiment_summary,
            sentiment_overall_csv,
            sentiment_spring_chart,
            sentiment_summer_chart,
            sentiment_autumn_chart,
            sentiment_winter_chart,
            sentiment_spring_pos_wc,
            sentiment_spring_neg_wc,
            sentiment_summer_pos_wc,
            sentiment_summer_neg_wc,
            sentiment_autumn_pos_wc,
            sentiment_autumn_neg_wc,
            sentiment_winter_pos_wc,
            sentiment_winter_neg_wc,
            sentiment_df_output,
            blog_results_df_state,
            blog_judgments_state,
            sentiment_blog_page_num_input,
            sentiment_blog_total_pages_output,
            sentiment_blog_list_csv,
            sentiment_individual_summary,
            sentiment_individual_donut_chart,
            sentiment_individual_score_chart,
            sentiment_blog_detail_accordion,
        ]
    )

    rank_facilities_btn.click(
        fn=rank_facilities,
        inputs=[recommended_facilities_state, ranking_reviews_slider, ranking_top_n_slider],
        outputs=[
            recommended_facilities_state,
            recommend_status,
            recommend_facilities_gallery,
            facility_ranking_report
        ]
    )

    rank_courses_btn.click(
        fn=rank_courses,
        inputs=[recommended_courses_state, ranking_reviews_slider, ranking_top_n_slider],
        outputs=[
            recommended_courses_state,
            recommend_status,
            recommend_courses_gallery,
            course_ranking_report
        ]
    )

    recommend_btn.click(
        fn=run_nearby_search,
        inputs=[selected_festival_details_state, recommend_radius_slider],
        outputs=[
            recommended_facilities_state, 
            recommended_courses_state,
            recommend_facilities_gallery, 
            recommend_courses_gallery, 
            recommend_status,
            ranking_controls
        ]
    )

    # Event handlers for pagination and individual blog details within sentiment analysis tab
    sentiment_blog_page_num_input.change(
        fn=lambda df, page_num: change_page(df, page_num),
        inputs=[blog_results_df_state, sentiment_blog_page_num_input],
        outputs=[sentiment_df_output, sentiment_blog_page_num_input, sentiment_blog_total_pages_output]
    )

    def handle_df_select(evt: gr.SelectData, page_num: int, df: pd.DataFrame, judgments: list):
        BLOG_PAGE_SIZE = 10
        page_num = page_num or 1
        
        global_idx = (int(page_num) - 1) * BLOG_PAGE_SIZE + evt.index[0]

        if df is None or df.empty or judgments is None or not isinstance(judgments, list) or global_idx >= len(judgments):
            return gr.update(), gr.update(), gr.update(), gr.update()

        judgments_for_row = judgments[global_idx]
        
        if not isinstance(judgments_for_row, list):
            return gr.update(), gr.update(), gr.update(), gr.update()

        donut_chart = create_donut_chart(
            sum(1 for j in judgments_for_row if isinstance(j, dict) and j.get('final_verdict') == 'ê¸ì •'),
            sum(1 for j in judgments_for_row if isinstance(j, dict) and j.get('final_verdict') == 'ë¶€ì •'),
            f"{df.iloc[global_idx]['ë¸”ë¡œê·¸ ì œëª©'][:20]}... ê¸/ë¶€ì • ë¹„ìœ¨"
        )

        score_chart = create_sentence_score_bar_chart(
            judgments_for_row,
            f"{df.iloc[global_idx]['ë¸”ë¡œê·¸ ì œëª©'][:20]}... ë¬¸ì¥ë³„ ì ìˆ˜"
        )
        
        summary_text = df.iloc[global_idx]['ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½']

        return gr.update(value=donut_chart, visible=True), gr.update(value=score_chart, visible=True), gr.update(value=summary_text, visible=True), gr.update(open=True, visible=True)

    sentiment_df_output.select(
        fn=handle_df_select,
        inputs=[sentiment_blog_page_num_input, blog_results_df_state, blog_judgments_state],
        outputs=[
            sentiment_individual_donut_chart,
            sentiment_individual_score_chart,
            sentiment_individual_summary,
            sentiment_blog_detail_accordion
        ]
    )

    def display_recommend_details(evt: gr.SelectData, recommended_results):
        if not evt or not recommended_results:
            return gr.update(visible=False), gr.update(visible=False)

        selected_item = recommended_results[evt.index]
        
        details = []
        exclude_cols = [
            'type', 'id', 'contentid', 'contenttypeid', 'lDongRegnCd', 'lDongSignguCd', 
            'lclsSystm1', 'lclsSystm2', 'lclsSystm3', 'mlevel', 'cpyrhtDivCd',
            'areacode', 'cat1', 'cat2', 'cat3', 'createdtime', 'mapx', 'mapy', 
            'modifiedtime', 'sigungucode', 'sub_points' # also exclude the sub_points list itself from the main loop
        ]

        # Handle main info for both facilities and courses
        for key, value in selected_item.items():
            if key in exclude_cols:
                continue
            if value is not None and str(value).strip() != '':
                display_key = COLUMN_TRANSLATIONS.get(key, key)
                details.append(f"**{display_key}**: {value}")

        # If it's a course, handle the sub_points specially
        if 'sub_points' in selected_item and selected_item['sub_points']:
            sub_points = selected_item['sub_points']
            
            all_subnames = [sp.get('subname') for sp in sub_points if sp.get('subname')]
            if all_subnames:
                # Remove duplicates while preserving order
                unique_subnames = list(dict.fromkeys(all_subnames))
                details.append(f"**ì„¸ë¶€ ì½”ìŠ¤ëª…**: {', '.join(unique_subnames)}")

            all_overviews = [sp.get('subdetailoverview') for sp in sub_points if sp.get('subdetailoverview')]
            if all_overviews:
                overview_list_str = [f"{i+1}. {desc}" for i, desc in enumerate(all_overviews)]
                details.append(f"**ì„¸ë¶€ ì½”ìŠ¤ê°œìš”**:\n" + "\n".join(overview_list_str))

        details_text = "\n\n".join(details)

        return gr.update(value=details_text), gr.update(visible=True, open=True)

    recommend_facilities_gallery.select(
        fn=display_recommend_details,
        inputs=[recommended_facilities_state],
        outputs=[recommend_details_output, recommend_details_accordion]
    )

    recommend_courses_gallery.select(
        fn=display_recommend_details,
        inputs=[recommended_courses_state],
        outputs=[recommend_details_output, recommend_details_accordion]
    )

if __name__ == "__main__":

    ALL_FESTIVAL_CATEGORIES = load_festival_categories_and_maps()
    
    demo.launch(allowed_paths=["assets"])